# Faster Pytorch

This repository is about speeding up Pytorch training. It is a collection of techniques that can be used to speed up Pytorch training. We will focus on their respective principles and how to use them with pytorch (although most of them will be used with pytorch lighting, which we will also introduce).

1. Monitoring with Wandb in [01Wandb](01Wandb/01.md)
2. DL framework Lightning in [02Lightning](02Lightning/02.md)
3. Distributed training with Deepspeed in [03Deepspeed](03Deepspeed/03.md)
4. Fully sharded training with Fairscale in [04Fairscale](04Fairscale/04.md)
5. Accelerating existing models with Fabric or Accelerate in [05FabricAndAccelerate](05FabricAndAccelerate/05.md)