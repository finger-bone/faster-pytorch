# Deepspeed for Accelerating Training

## Introduction

ZeRO (Zero Redundancy Optimizer) is a memory optimization technique that allows training models with billions of parameters on a single GPU. It is a combination of three techniques: ZeRO-Offload, ZeRO-Stage, and ZeRO-Memory. 

ZeRO-Offload is a technique that offloads optimizer states to the host CPU.

ZeRO-Stage is a technique that partitions the optimizer states across the GPUs.

ZeRO-Memory is a technique that partitions the model across the GPUs.

Deepspeed is a library that implements ZeRO optimization for PyTorch. It provides a simple API to enable ZeRO optimization in your PyTorch model. It also supports other optimizations like mixed precision training, gradient accumulation, and more.

## Construct Another Model

The previous model was not very complex and it was fast to train even without any optimization. Let's construct a more complex model to see the effect of Deepspeed.

The task here is IMDB sentiment analysis. IMDB input is a movie review and the output is the sentiment of the review, positive or negative.

### Boot Up Logger

We will use Weights and Biases for logging.

```python
run = wandb.init(
    project="demo",
    name="imdb-demo",
    tags=["demo"],
    config={
        "batch_size": 8,
        "lr": 0.0001,
        "max_epochs": 5,
        "dropout": 0.1,
        "gemma": 0.99,
        "lr_step": 1,
        "embed_dim": 512,
        "num_heads": 8,
        "num_layers": 6,
        "max_len": 2048,
        "hidden_dim": 2048,
        "num_workers": 8,
    }
)
logger = WandbLogger(run)
```

### Data Preparation

We will use the Hugging Face NLP tool chains. First, initialize the tokenizer, we use `FacebookAI/roberta-large`.

```python
from transformers import RobertaTokenizer
tokenizer = AutoTokenizer.from_pretrained("facebook/roberta-large")
```

We define the dataset in a separate file or else there would be a multi-process hitch.

```python
import torch
from torch.utils.data.dataset import Dataset
from transformers import PreTrainedTokenizer
from torch import Tensor

class ImdbDataset(Dataset):
    
    def __init__(self, hf_dataset, tokenizer: PreTrainedTokenizer):
        super(ImdbDataset, self).__init__()
        self.data = hf_dataset
        self.tokenizer = tokenizer
        
    def __getitem__(self, index: int):
        return (
            Tensor(
                self.tokenizer.encode(
                    (self.data[index]["text"]).replace("<br/>", ""),
                )
            ).to(dtype=torch.long),
            Tensor([self.data[index]["label"]]).to(dtype=torch.long)
        )

    def __len__(self) -> int:
        return len(self.data)
```

Then define the lightning module, under the same file as the dataset.

```python
class ImdbDataModule(pl.LightningDataModule):
    
    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        batch_size: int,
        num_workers: int,
    ):
        super(ImdbDataModule, self).__init__()
        self.tokenizer = tokenizer
        self.batch_size = batch_size
        self.num_workers = num_workers
        
    def prepare_data(self):
        dataset = load_dataset("stanfordnlp/imdb", cache_dir="./data/")
    
    def setup(self, stage: str) -> None:
        dataset = load_dataset("stanfordnlp/imdb", cache_dir="./data/")
        test = dataset["test"]
        test = test.train_test_split(test_size=0.5)
        dataset["test"] = test["train"]
        dataset["validation"] = test["test"]
        
        if stage == "fit" or stage is None:
            self.train_data = ImdbDataset(dataset["train"], self.tokenizer)
            self.val_data = ImdbDataset(dataset["validation"], self.tokenizer)
        else:
            self.test_data = ImdbDataset(dataset["test"], self.tokenizer)
        
        def collate_fn(batch):
            x, y = zip(*batch)
            return torch.nn.utils.rnn.pad_sequence(x, batch_first=True), torch.cat(y)

        self.collate_fn = collate_fn
    
    def test_dataloader(self):
        return DataLoader(
            self.train_data,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            num_workers=self.num_workers,
        )
    
    def val_dataloader(self):
        return DataLoader(
            self.val_data,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            num_workers=self.num_workers,
        )
    
    def train_dataloader(self):
        return DataLoader(
            self.train_data,
            batch_size=self.batch_size,
            collate_fn=self.collate_fn,
            num_workers=self.num_workers,
        )
```

### Model Setup

This is a simple transformer encoder only model for classification.

```python
class ImdbModel(pl.LightningModule):
    
    def __init__(
        self,
        tokenizer: PreTrainedTokenizer,
        embed_dim = 512,
        hidden_dim = 2048,
        max_len = 1024,
        num_heads = 8,
        num_layers = 8,
        dropout = 0.1,
    ):
        super(ImdbModel, self).__init__()
        self.tokenizer = tokenizer
        self.embed = nn.Sequential(
            nn.Embedding(tokenizer.vocab_size, embed_dim),
            PositionalEncoding(embed_dim, dropout, max_len),
        )
        self.encs = nn.ModuleList(
            nn.TransformerEncoderLayer(
                d_model=embed_dim,
                nhead=num_heads,
                dropout=dropout,
            )
            for _ in range(num_layers)
        )
        self.classifier = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 2),
        )
        
        self.loss_fn = nn.CrossEntropyLoss()
    
    def forward(self, x):
        x = self.embed(x)
        for enc in self.encs:
            x = enc(x)
        x = x.mean(dim=1)
        return self.classifier(x)

    def configure_optimizers(self):
        optim = torch.optim.Adam(self.parameters(), lr=wandb.config.lr)
        sched = torch.optim.lr_scheduler.StepLR(optim, step_size=wandb.config.lr_step, gamma=wandb.config.gemma)
        return [optim], [sched]

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("val_loss", loss)
        acc = (y_hat.argmax(dim=1) == y).float().mean()
        self.log("val_acc", acc)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss_fn(y_hat, y)
        self.log("test_loss", loss)
        acc = (y_hat.argmax(dim=1) == y).float().mean()
        self.log("test_acc", acc)
        return loss
```

### Speed Monitor

Define a call back to setup a speed meter for the model.

```python
from typing import Any
from pytorch_lightning import LightningModule, Trainer
from time import time

class SpeedCounterCallback(pl.Callback):
    
    def on_train_batch_start(self, trainer: Trainer, pl_module: LightningModule, batch: Any, batch_idx: int) -> None:
        self.batch_start_time = time()
    
    def on_train_batch_end(self, trainer: Trainer, pl_module: LightningModule, outputs: Any, batch: Any, batch_idx: int) -> None:
        self.batch_end_time = time()
        wandb.log({"second_per_batch": self.batch_end_time - self.batch_start_time})
```

### Train the Model

Since we are using multi-processing data loader, we need to wrap the model with the python-styled main function.

```python
if __name__ == "__main__":
    trainer = pl.Trainer(
        max_epochs=wandb.config.max_epochs,
        logger=logger,
        callbacks=[
            SpeedCounterCallback(),
        ],
    )
    model = ImdbModel(
        tokenizer,
        run.config.embed_dim,
        run.config.hidden_dim,
        run.config.max_len,
        run.config.num_heads,
        run.config.num_layers,
        run.config.dropout,
    )
    from imdb_dataset import ImdbDataModule
    data_module = ImdbDataModule(
        tokenizer,
        run.config.batch_size,
        run.config.num_workers,
        run.config.max_len
    )
    trainer.fit(model, data_module)
```

This model train takes about an hour to train per epoch on my MacBook Pro, roughly more than one second for each batch of size eight, which is way slower than the previous model. Using this model can we better illustrate the effect of acceleration.

